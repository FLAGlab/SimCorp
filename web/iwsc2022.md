# Evaluation

We evaluate our program similitude analysis in two stages. First, we validate the effectiveness our approach and compare its performance with respect to existing Type-4 clone detection approaches from the literature. Second, we focus on the evaluation of our approach using a new corpus for program similitude analysis, to evaluate our algorithm on larger codebases. The programs in the corpus are divided in 5 domains, including for each domain implementations that are alike and that are structurally different --that is, differ further than small deltas, but still provide the same behavior.

## Code Clone Detection

We validate the effectiveness of our approach with respect to the state-of-the-art through the XXXX benchmark focusing on Type-4 code clone detection.

### Evaluation Environment

Our environment for the evaluation benchmark uses macOS Monterrey withe the Apple M1 chip with 8GB RAM. Our evaluation uses version 3.3 of LLVM and C++11.

## Usefulness in Known Domains

## Similitude Analysis Corpus

To further evaluate our approach, we analyze more complex programs from different domains using different programming styles. With this purpose we build a corpus for the similitude analysis of different programs.

### Evaluation Environment

To execute all the evaluation scenarios we use a Intel core i5-5257U processor with 8GB RAM running Ubuntu 18.04.2 LTS. Our evaluation uses version 3.3 of LLVM and C++11. 

### Data Corpus

Our corpus consists of 566 different C++ programs extracted from the 
Codeforces competitive programming online judge. The extracted 
programs are solution submissions to five problems (domains).
The problems used present 
different implementation characteristics, ranging from simple 
straightforward implementations (the difficulty level of the problems 
is given by their accompanying letter starting with A as the simplest 
problem), to implementations using multiple 
functions, requiring to manage complex data structures (\eg DSU) or 
advanced algorithms (\eg dynamic programming (dp), or computational 
geometry). For each of the problems we extracted up to 50 submissions 
for each category:
\begin{enumerate*}[label=(\arabic*)]
\item submissions that complete all test cases (OK),
\item submissions that yield a runtime error (RUNTIME\_ERROR), and 
\item submissions that do not solve the problem 
properly (WRONG\_ANSWER). 
\end{enumerate*}
\fref{tab:codeforces} shows the distribution of the data set 
classified by solution category, each containing
the average \ac{LOC} per submission.

\begin{table}[hptb]
  \centering
  \caption{Corpus of Codeforces programs}
  \label{tab:codeforces}
  \input{tables/codeforces}
\end{table}

An important characteristic of the data set is that all OK solutions 
for a problem are assured to provide the same output to the same 
input. However, there is a wide disparity on the 
submissions for the other two categories, RUNTIME\_ERROR and 
WRONG\_ANSWER. In these cases, the submitted solutions may variate 
from empty programs, to programs close to a solution, to programs that 
solve completely different problems.

### Experiment Design

The similarity evaluation of the programs in our corpus first 
computes a similarity matrix containing the similarity score for 
every pair of programs compared, as explained in 
\fref{sec:algorithm}. 
We then use \ac{PCA}~\cite{jollifee86} to reduce the dimension of 
the matrix to two principal components, and plot these components 
using the \ac{PCA} index. 
Furthermore, we use the silhouette coefficient~\cite{rousseuw87} on 
the similarity matrix to evaluate the cohesion and separation of 
the clusters per problem. The silhouette score is bounded between 
$1$ and $-1$, similar programs have a score close to 1, overlapped 
clusters have a score close to 0, and dissimilar programs have a 
negative score.

We evaluate our algorithm using the LAV similarity 
analysis~\cite{vujovsevic13} as a baseline. We take this baseline 
for our experiments, as this constitutes the state-of-the-art 
analysis for node-based similarity, which is closest to our approach.
Other semantic analysis approaches in the literature are evaluated on small delta variations of the base-code, which are unfit for our purpose.  
Here we focus on the comparison of functionally equivalent programs that satisfy problems' conditions (OK), which we use to identify different implementation 
techniques for a specific problem.
In the evaluation we first compare all available programs in the 
cluster (each represented by a color in Figures 
\ref{fig:all-analysis} to 
\ref{fig:runtime-all}), with the 
objective of observing whether our algorithm is capable of 
differentiating between the different problem submissions.
We then focus in the comparison by type of submission (OK, 
RUNTIME\_ERROR, or WRONG\_ANSWER) to observe the relevance of 
functional equivalence of programs when analyzing their similitude.
Finally, we use our algorithm to identify different implementation 
techniques from a given set of submissions for a specific problem.

Note that all programs in our corpus have common a behavior 
(\eg I/O instructions), and therefore have a positive similarity 
score. Furthermore, as all the submissions that solve a problem 
(\ie the OK category) have the same black-box behavior, we expect all 
solutions to a problem to be clustered. However, while the solutions 
to a problem behave the same, the algorithms used can have different 
implementations; therefore, we also expect to find sub-clusters for 
each problem. 
The case of incorrect solutions to the problems is not as well 
defined. These solutions may come from quite different algorithms in 
terms of the underlying solution strategy, structure, and data 
structures used, so not all solutions should be close together. 

## Results

Our evaluation calculates the similarity of the 
different problems from three perspectives. First, we generate the 
similarity matrix using the LAV method. %(labeled ORIGINAL in the figures). 
Second, we use our node similarity method to compare the submissions 
(labeled ORIGINAL-NODE-SYM in the figures). 
Third, we applied the normalization on the node similarity as 
described in \fref{sec:algorithm}. %(labeled NORMALIZED-NODE-SYM in the figures).

Our first experiment compares all 566 available programs, generating 
a $566 \times 566$ similarity matrix. The LAV evaluation in 
\fref{fig:similarity-original} shows some clustering of the 
submissions for each problem (identified by shape and color of the 
point in the figure). 
In particular, we observe scattering for all problems, 
predominately for problem 1579A, and a lot of overlapping between 
the submissions for the different problems, for example 922E, 558B, 
and 1553G. The overlapping of the clusters is confirmed using the 
silhouette score. In the case of the ORIGINAL similarity 
measure in \fref{fig:similarity-original}, we obtained a silhouette score of $0.14$.
Using the improved similarity method we obtain a similar clustering as 
before, shown in \fref{fig:similarity-node-sym}, ORIGINAL-NODE-SYM, 
with a slight better silhouette score of $0.15$. Finally, using our 
proposed approach, NORMALIZED-NODE-SYM in 
\fref{fig:similarity-normalized}, we observe the clusters are less 
defined, and the silhouette index decreases to $0.057$.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{\columnwidth}
  	\includegraphics[width=\columnwidth]{figures/similarity-ORIGINAL}
  	\caption{ORIGINAL algorithm}
  	\label{fig:similarity-original}
  \end{subfigure}

  \begin{subfigure}[t]{\columnwidth}
%  \centering
	  \includegraphics[width=\columnwidth]{figures/similarity-ORIGINAL-NODE-SYM}
	  \caption{ORIGINAL-NODE-SYM algorithm}
	  \label{fig:similarity-node-sym}
  \end{subfigure}

  \begin{subfigure}[t]{\columnwidth}
%  \centering
	  \includegraphics[width=\columnwidth]{figures/similarity-NORMALIZED-NODE-SYM}
	  \caption{NORMALIZED-NODE-SYM algorithm}
	  \label{fig:similarity-normalized}
	\end{subfigure}
	\caption{PCA-x and PCA-y components for all types of submissions for all problems}
	\label{fig:all-analysis}
\end{figure}

This experiment supports our hypothesis that submissions to 
the same problem should be clustered together, as shown in the 
figures. However, we can attribute the low scores obtained to two 
factors. First, the structure of 
competitive programming solution programs are very well defined, 
always containing a block of code in charge of reading the information 
from standard input, a block solving the problem, and a block writing 
the solution to standard output. As a consequence, solutions across 
problems will have \ac{CFG} nodes that naturally match, causing them 
to increase the similarity score between programs, as seen in the 
overlap between the submissions.
Second, as we are comparing all submissions for all problems, we may 
take into account solutions to problems that are completely different 
from correct solutions (\eg RUNTIME\_ERROR or WRONG\_ANSWER). This 
can cause the sparsity of the solutions for particular problems. 

In response to these observations, we then analyze the submissions 
of a single type. 
When analyzing correct solutions (OK) to the 
problems, %as shown in \fref{fig:ok-all}. 
\fref{fig:similarity-original-ok} shows some clustering for each problem (identified by shape and marker's color in the figure). 
The silhouette score is $0.234$, but still presents 
overlapping and scattering between problems 1579A, 922E, and 1553G. 
Using our algorithm, \fref{fig:similarity-normalized-ok} 
shows a better clustering, %comparable to that of the LAV method, 
with 
a silhouette score of $0.204$, and a $3.57\times$ improvement over 
the evaluation of all problems. This suggests that, as 
problems have behavior equivalence, our approach 
significantly improves the similarity metric of the evaluated 
programs. 
We also analyze all RUNTIME\_ERROR submissions for both 
the ORIGINAL method, shown in \fref{fig:similarity-original-runtime}, 
and the NORMALIZED-NODE-SYM method, shown in 
\fref{fig:similarity-normalized-runtime}. The silhouette scores for 
each method are $0.050$ and $0.002$, respectively. In this case, as in 
the comparison of all problem submissions, we see an overlapped 
clustering for our approach, in particular with problems 558B and 
1579A. As before, we can attribute the scattering of the submitted 
solutions to the general structure of the programs for competitive 
programming capturing input/output interaction. The case of these two 
problems in particular is that they share a common algorithm which first cycles over the structures and use a conditional to verify the 
required property by the problem statement. This pattern, however 
incorrect, is repeated in many of the submissions for both problems.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{\columnwidth}
  	\includegraphics[width=\columnwidth]{figures/similarity-ORIGINAL-OK}
  	\caption{ORIGINAL algorithm for OK submissions}
  	\label{fig:similarity-original-ok}
  \end{subfigure}

  \begin{subfigure}[t]{\columnwidth}
  	\includegraphics[width=\columnwidth]{figures/similarity-NORMALIZED-NODE-SYM-OK}
  	\caption{NORMALIZED-NODE-SYM algorithm for OK submissions}
  	\label{fig:similarity-normalized-ok}
  \end{subfigure}
  \caption{PCA-x and PCA-y components for all problems}
  \label{fig:ok-all}
\end{figure}

\begin{figure}
    \begin{subfigure}[t]{\columnwidth}
  	\includegraphics[width=\columnwidth]{figures/similarity-ORIGINAL-RUNTIME_ERROR}
  	\caption{ORIGINAL algorithm}
  	\label{fig:similarity-original-runtime}
  \end{subfigure}

  \begin{subfigure}[t]{\columnwidth}
	\includegraphics[width=\columnwidth]{figures/similarity-NORMALIZED-NODE-SYM-RUNTIME_ERROR}
  	\caption{NORMALIZED-NODE-SYM algorithm}
	\label{fig:similarity-normalized-runtime}
  \end{subfigure}
  \caption{PCA-x and PCA-y components for RUNTIME\_ERROR submissions for all problems}
  \label{fig:runtime-all}
\end{figure}

Note program scattering can be attributed to 
specific algorithms. There might be different 
solutions for a given problem, therefore these solutions should not be 
as similar to each other, as other solutions with the same algorithmic 
principle. 

To test this hypothesis, we analyze the correct solutions (OK).
\footnote{Similar analyses for all the problems and submission types in the corpus is available at our online repository.} 
\fref{fig:similarity-original-558b-ok} shows the OK submissions for 
problem 558B using the ORIGINAL method, with two clusters, showing two 
distinctive solutions to this problem. From the figure we see a tight 
cluster represented by the solutions using a circle, and a more 
scattered clustered represented by the solutions using a 
{\color{orange} \textbf{$\times$}}. This explains the low 
silhouette score of $0.276$.
\fref{fig:similarity-normalize-558b-ok} shows our NORMALIZED method 
evaluating the same problem. Here we too obtain two distinctive 
clusters. In this figure we observe that the clusters are less 
scattered, explained by a silhouette score of $0.483$. As there are 
two common solution patterns for the problem, we confirm our algorithm 
is effective in finding similar and dissimilar programs for 
particular problems with common black-box behavior. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{\columnwidth}
  	\includegraphics[width=\columnwidth]{figures/similarity-ORIGINAL-558-B-OK}
  	\caption{ORIGINAL algorithm for problem 558B OK}
  	\label{fig:similarity-original-558b-ok}  
  \end{subfigure}

  \begin{subfigure}[t]{\columnwidth}
  	\includegraphics[width=\columnwidth]{figures/similarity-NORMALIZED-NODE-SYM-558-B-OK}
  	\caption{NORMALIZED-NODE-SYM algorithm for problem 558B OK}
  	\label{fig:similarity-normalize-558b-ok}
  \end{subfigure}
  \caption{PCA-x and PCA-y component evaluation}
  \label{fig:558b}
\end{figure}

Similar to this specific analysis, 
We use our approach to evaluate   
submission types for all the problems in the corpus. 
\fref{tab:silhouette} presents the silhouette scores for all the 
programs in our corpus, with the best scores in bold. It 
is important to note, that the silhouette score for a specific 
problem, represents the ability of the algorithm to detect different 
implementations for the same problem. Not all problems 
contain different implementations within the same type of submission. 
Identifying such property is valuable for applications domains like 
diversity or N-version programming.

\begin{table}[hptb]
  \centering
  \caption{Silhouette score for Codeforces programs}
  \label{tab:silhouette}
  \input{tables/silhouette}
\end{table}

## Discussion

Our experiments help us validate the effectiveness of
our algorithm to measure the similitude between different programs. 
Based on the validation we conclude our approach is appropriate to: 
\begin{enumerate*}[label=(\arabic*)]
\item Identify features common to different problems, for example, 
in the case of the Codeforces corpus, in which 
we identify similarities across all analyzed submissions in the way 
the input and output of the problem are processed (independent of the 
specific instructions used).
\item Detect differences in the algorithms behind different programs, 
even when they are functionally equivalent. % (\ie have the same output for the same inputs). 
This is shown in the clusters for the correct 
(OK) solutions to a problem in our corpus.
\end{enumerate*}

The performance of our algorithm, measured using the silhouette score, 
is similar to the performance of the baseline, \ie the LAV algorithm. 
Our evaluation shows that the use of our node similarity definition 
has a slight improvement over the baseline in most cases. Moreover, 
when analyzing specific algorithm pairs identified as 
similar/disimilar, we respectively note a great resemblance/disparity 
in the specific implementations. This is beneficial as a notion of 
diversity between algorithms. However, the validation shows that in 
some cases using node normalization is detrimental to the performance. 
We note that, as programs differ but have an important 
feature in common (\eg input/output processing), the 
algorithm will detect these as similar, decreasing the silhouette 
score.

